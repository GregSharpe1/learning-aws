# Simple storage service (S3) 101

S3 providers developers and IT teams with secure, durable, highly-scalable object storage. Amazon S3 is easy to use, with a simple interface to store and retrieve any amount of data from anywhere on the web.

## What is S3

* S3 is a safe place to store your files
* Object-based storage - Meaning that you'd save files to the storage not run anything from the storage, eg an operating system
* The data is spread across multiple devices and facilities

## Basics of S3

* Object based storage
* Files can be from 0 to 5TB in size
* There is unlimited storage
* Files are stored in Buckets (think of a directory in the cloud)
* S3 is a universal namespace. That is, names must be unique globally.
* example s3 bucket link: https://s3-aws-region.amazonaws.com/bucket-name
* when uploading always return a `200` return code

## Data consistency model for S3

* Read after write consistency for PUTS of a **NEW** object
* Eventual consistency for overwrite **PUTS** and **DELETES** can take some time to propagate

## S3 is a simple key-value store

* S3 is object based. And is made up of the following:
  * Key (name of the object)
  * Value (the data itself)
  * Version ID (important for versioning)
  * Metadata (data on data)
  * Sub-resources:
    * Access Control Lists (ACL)
    * Torrents

## Basics continued

* Built for 99.99% availability for the S3 platform
* Amazon Guarantee 99.9% availability
* Amazon guarantees 99.99999999999% durability for S3 information (11 x 9s)
* Storage classes are available
* Lifecycle management (eg, if a file is over 30 days over move)
* Versioning
* Encryption (multiple types)
* Secure your data using Access
* Versioning MFA delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security.
* Control lists and buckets policies

## Storage tiers/classes

* **S3 Standard**: 99.99% availability, 99.999999999% durability, stored redundantly across multiple devices in multiple facilities, and is designed to sustain the loss of **2** facilities concurrently.
* **S3-IA**: Infrequently accessed. For data that is accessed less frequently, but requires rapid access when needed. Lower fee than standard S3, but you are charged a retrieval fee.
* **S3 One Zone-IA**: **lower cost** option for infrequently accessed data, but do not require the multiple availability zone data resilience
* **Glacier**: Very cheap, but used for archival only. Expedited, standard or bulk. A standard retrieval time takes 3 - 5 hours

## Charges/Billing

* Charged for:
  * Storage
  * Requests
  * Storage management pricing (metadata)
  * Data transfer pricing (cross region replication)
  * Transfer acceleration
    * What is Transfer acceleration? Amazon S3 TA enables fast, easy and secure transfers of files over long distances between your end users and an S3 bucket. TA takes advantages of Amazon's Cloud front CDN network.

## Exam tips

* Read the S3 FAQs before taking the exams
* S3 buckets
  * Encryption
    * Client side encryption
    * Server side encryption
      * Server side encryption with Amazon S3 Managed Keys (SSE-S3)
      * Server side encryption with KMS (SSE-KMS)
      * Server side encryption with Customer Provided Keys (SSE-C)
  * By default all private and all objects stored instead them are private
  * Control access to buckets using ACL (access control list) or Bucket policies
  * Once you've activated versioning, you can only suspend not completely remove
  * Public permissions are granted on an individual upload basis (v1 could be public, but v2 won't be without specifically granting public access again)

## Cross region replication (CRR)

Why would you want to replicate buckets?

First all, replication is a form of safety. Implementing such a measure will only benefit you having more copies of the files and therefore more safety. Another major reason why you'd replicate objects, especially into a completely separate region would be delivery to a customer. Placing the files closer to the consumer will decrease latency.

* Two buckets, the first being the bucket you upload your content to.
* The second bucket being set to replica
  * Must enable versioning
  * Can change storage class
  * If objects are already within the first bucket, they will not migrate by default, CRR will only replicate **new** objects
  * To copy objects from bucket to bucket use the CLI tool
  * If you delete a files within the primary bucket, it will **NOT** be deleted within the replica buckets

## Lifecycle Management

Why use Lifecycle management? Well, cost. Paying the cost of S3 standard tier can be expensive, so with lifecycle management you can move files that typical won't need to be frequently accessed to a cheaper S3 solution (S3-IA) or even Amazon Glacier option.

* You do have to have versioning on for buckets with Lifecycle configurations
* Lifecycle management can be applied to previous and current versions

## Securing S3 Buckets

* By default, all S3 buckets are **PRIVATE**
* There are two separate ways of setting up access control to your buckets
  * Bucket policies
    * These policies are bucket wide, meaning that the level of control is applied to the bucket as a whole and cannot be applied to individual objects
  * Access control lists
    * These control lists can be applied to individual files, meaning that you can permit public access to one file within a bucket and still have the others be private.
* S3 buckets can be configured to create access logs which log all requests made to the S3 bucket. These logs can be committed to another S3 bucket within the same account **OR** committed to a separate S3 bucket within a *completely* different account

### Encryption

There are four different methods of encryption **within** S3.

* In transit - when you're sending info to and from the S3 bucket (with SSL and TLS)
* At Rest
  * Server side encryption
    * S3 managed keys - SSE-S3 - Each object is encrypted by an Amazon generated key
    * AWS Key Managed Service, Managed Keys - SSE-KMS - By using this service it provides you with an audit trail on when the Keys were used to decrypt an object within S3
    * Server side encryption, with a customer provided keys - SSE-C - customer manages the encryption key
  * Client side encryption - where the custom encrypts the data before sending it to S3

## Amazon Storage Gateway

AWS Storage Gateway is a service that connects an on-premises software application with cloud-based storage to provide seamless and secure integration between an organisation's on-premises IT environment and AWS's storage infrastructure. This service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.

AWS Storage Gateway's software application is available for download as a vm image that you install on a host in your datacenter.

### Four types of storage gateways

* File gateway (NFS) - Flat files stored directly within S3
* Volumes Gateway (iSCSI) - Block based storage (can running VM's type of files)
  * stored volumes
  * cached volumes
* Tape gateway (VTL) - Create Virtual tapes and backup to S3

## Snowball

Using snowball, eliminates the common challenges with large-scale data transfers including high network costs, long transfer times and security concerns. Transferring data with snowball is simple, fast, secure, and can be little as one-fifth the cost of high-speed Internet. **petabyte-scale** data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS.

### Import/Export

AWS Import/Export disk accelerates moving large amounts of data into and out of the AWS cloud using portable storage devices for transport. AW Import/Export disk transfers your data directly onto and off of storage devices using Amazon's high-speed internal network and bypassing the internet.

### Types of Snowballs

* Snowball - Just onboard storage up to 80TB
* Snowball edge - Has up to 100TB storage capacity and **compute** capabilities, you can also run lambda functions from these devices. Thus enabling you to have compute capacity to places where you would not normally be able to do so (can be deployed onto Aeroplanes, as offline compute).
* Snowmobile - massive container for peta-byte, exeo-byte of storage capacity.

## Exam tips for Snowball

* Understand what snowball is
* Understand what Import/Export is
* What can as snowball can do
  * Import to S3
  * Export from S3

## S3 Transfer Acceleration

S3 Transfer acceleration utilises the CloudFront Edge Network to accelerate your uploads to S3. Instead of uploading directly to your S3 bucket, you can use a distinct URL to upload directly to an edge location which will then transfer that file to S3.

* Hint: s3 accelerate endpoint will be in the following format: <bucket-name>.s3-accelerate.amazonaws.com
